> 转载自：<https://mp.weixin.qq.com/s?__biz=MzA3MDA2MjE2OQ==&mid=402237490&idx=1&sn=799db0a7f86cee22db9ba8bfbe2ead1a&scene=1&srcid=0310F01IiV32eYqfoiYnioC2>

#### 今日话题

敏感关键词过滤

1. 以前做过一个评论项目，把敏感关键词存入redis，然后通过php提供api，再把提交的评论内容拆分词组去请求api校验 —挨踢生涯

2. 最难的应该是去掉无用的内容吧 特殊符号什么的 —Vergil Weng

3. HTTPCWS – 基于HTTP协议的开源中文分词系统 以前我用的是这个做分词 —挨踢生涯

4. 比如还有正文加关键词链接，关键词很多的情况，怎么弄？ —蛮-com

5. 同一个关键词只出现一次 以前用scws做的，一个php的扩展。提取词性和权重，取前多少个 在用preg_replace做超链替换 —青衫隐_刘

6. 特殊字符不知道 用 贝叶斯能解决不 —@理鱼

7. 给你一个思路，火星文转拼音，再加权 — 飛魚

8. 前缀树 多脏词匹配 加一定的语境分析 前缀树 是核心 深入还是带点机器学习就完美了 难的是 脏词更新  
底层是golang 之前用php lua 实现过 效率不行  
前缀树，但和传统前缀树不一样。 数据结构上 要针对 多脏词匹配 进行一定的设计。 然后就是。 这是基础  
外围的主要就是 干扰字符 过滤 简繁体转换  
外带一定的语境分析。有些词语 比如营销的。 单独存在是没问题的。 但 如果 营销词 + 电话 或者联系方式 那就是 营销广告了  
然后就是脏词库了~ 我是网上找了基础的脏词库。然后 在日常拦截中 不断优化 而来  
哦还有一个就是 脏词 也要分 层级。。 目前我设计的 是 灰名单 和 黑名单 。。比如 习近平 就是灰名单的。。  
--小明

9. 可能一次需要考虑多种组合的情况 —Black  
比如，单独出现宋祖英没问题，要是文本里宋祖英前后又出现江泽民，就认为一个有问题，直接送待审核
这个多长文本里面出现就比较微妙，需要按照业务去处理 比如博客和微博处理方式不同 需要匹配多个词语是否存在，就是多模匹配
技术实现，golang有指针，用trie树好起来就比较简单 c/c++也行 —Black

10. trie本来就简单，trie的查找是线性的，取决于字符串长度，但很费内存 —陈亦

11. 双数组trie树 —Song

12. 针对中文还有个问题 必须转成拼音 不然这个前缀叔没法构造 —Black

13. http://my.oschina.net/goal/blog/201674 这是我之前用JS实现的。。。。当时转了Utf8，其实用Unicode就好了 --陈亦

14. 有什么好的方式方法么。感觉贝叶斯会好些....我们现在都是简单的字词匹配.... 有误差 准确率还低 - Nemo

15. 如果要过滤的句子有重复发送的特点可以通过检测连续发送句子的相似度,对方用多音字或火星文来变换怎么办？ - 陈竑

16. 简单点的就是直接匹配替换，但解决不了混淆。我们现在用随机森林，ac自动机等算法，模糊过滤大批量自动刷的,垃圾样本匹配 - 林志勇

17. 敏感词过滤和垃圾过滤是不一样的，敏感词是有限穷举,但是垃圾过滤，设计到模式匹配了,如果是重复计算，可以 用google的simhash,可以算出相似度,我们之前做文章抓取的时候，是用simhash判断相似的 - tiyee

18. 用拼音和偏旁来过滤 - timedcy

19. 句子的相似度要怎么判断？答： 直接对同一个用户连续发送的评论做余弦相似度计算 - 陈竑

20. 过滤词这块，我一直没有找到合适的处理方案，敏感词的字段格式以及非utf8格式的字怎么处理？ - 李水祥

21. ac自动机是做英文的吧，对中文效率挺低的啊 ,都是整英文的，因为英文基本组成就26字母 - tiyee

22. 中文用拼音呗,一般找个最长串，跟切词差不多 - Black

23. 为何不用Unicode进行二分法查找呢?  
详解：关键词过滤，无非是扫描内容，然后去字典匹配，一般英文的字典用trie tree就行了，几乎所有的demo几乎都是讲的英文trie，但是中文并不合适，中文的词是由字组成的，字本质上就是一个编码，那么，将编码相加，和放到一个数组里，数组里每个值对应的是unicode和相同的词，这样就能通过二分法找到unicode和所在的数组值，然后找到这个值对应的所有的词，然后从所有词里找出具体要找的词，为了减少相同的unicode和，可以采取加权算法，1.第一个字unicode+2.第二个字的unicode+3.第三个字的unicode...你说的转拼音，但是汉字并不能通过数学计算直接得出拼英，还得去另外一个拼音表里找出字的拼音 - tiyee  
trie树，保存unicode码 - 陈亦  
tiyee 的那个思路是干嘛？怎么搞成hash表的样子？是要做什么？英文能做trie tree ，中文就转换成unicode，每个字对应16bit或者32bit的int，然后去做trie tree不是可以吗？中文分词都是这么干的。效率为啥不行？ - 吴延毅  
中文的字太多了,如果按字做节点，节点数太庞大 - tiyee  
没你想的庞大，中文分词都这么搞。 - 吴延毅  
大概就是把中文单个字变成一个数字，然后把数字存储到trie节点，然后因为数字方便比较大小，还占空间小 - Black  
理论上，一个词平均三个字，一共4000字,那么就是4000 * 4000 * 4000节点 - tiyee    
占用内存大小取决于词库大小。你的词库一般能有多大？ - 吴延毅  
这个数字生规则可以是把unicode值得转，因为能够保证唯一性,相当于一个hash表,@tiyee 这么理解你的意思对吧 - Black  
不要想复杂了。unicode就代表一个字符。。。字符怎么会重复 - 陈亦  
我的意思是，把一个词所有的字的unicode生成一个int，尽量保证这个int的唯一性（但不绝对保证），然后根据二分法直接找到这个词 - tiyee  
"把一个词所有的字的unicode生成一个int" 这句怎么理解？ - 陈亦  
就是一个数学算法 - tiyee  
明白了。。。类似于信息指纹 - 陈亦  
1.第一个字unicode+2.第二个字的unicode+3.第三个字的unicode... - tiyee  
tiyee 的这句:那么就是4000*4000*4000节点 - 吴延毅  
tiyee说的没错啊。。。每个节点层都有所有可能 - 陈亦  
是错的，trie树关键是词库,是有所有可能，但是实际上都很稀疏。 - 吴延毅  
对的 - 陈亦  
tiyee的那个思路就是对unicode数组进行类似求hash值，然后二分去查找。个人不理解这个做法，因为目的似乎就是为了查找同一个unicode词对应的中文词(比如utf8编码)，这不就是一个map搞定吗？怎么那么复杂？ - 吴延毅  
map也得设计啊 ,你怎么设计map呢 ? - tiyee  
假设我的map就是开链hash表。也就够了。 - 吴延毅  
就是单链表吧 - 陈亦  
我的这种算法，就是模拟hash算法 - tiyee  
引入二分查找性能反而很可能更差,取决于hash表的buckket数量。 - 吴延毅  
就是以空间换时间了... - 陈亦  
你怎么去实现这个hash ? - tiyee  
累加取模。 - 吴延毅  
hash的算法明显比我的生成int算法复杂的多 - tiyee

24. http://my.oschina.net/goal/blog/200596 简单粗暴 - 陈亦

25. 词库的内存真心被夸大了，现在线上词库，中文分词时上千万的词库量都没事。除非实现的很差劲。以我经验来说，我们线上使用的cppjieba分词就是把utf8中文转换成unicode，然后对每个字作为trie的key去实现。何况敏感词过滤中，敏感词的词数应该远少于中文词库。内存个人认为不是问题。重点在于脏词挖掘(脏词的变形)和上下文关联(比如刚才的江泽民和宋祖英)。 - 吴延毅

26. 这个是自然语言处理范畴，比如去年季逸超 公布的magi,还有就是防水功能，有些无意义的文章的识别 - tiyee

27. 还有各种什么trie树，比如actrie之类我也体会不到优势有多大，我之前把cppjieba的trie改成了标准actrie实现，几乎没有性能提高(这么说可能有点骇人听闻，cppjieba是我写的，changelog里面应该有提过尝试过actrie)。个人分析是因为当词库的每个词平均长度比较短的时候，actrie起到的优化程度很小的缘故(当然也可能是我的实现太渣的缘故)。 - 吴延毅  
@吴延毅 结巴分词是你写的？ - 陈亦  
结巴分词的C++版本是写我写的 https://github.com/yanyiwu/cppjieba - 吴延毅  
http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html 这个是性能对比的文章 - 吴延毅  
那个jiebago实现得有问题。gojieba和nodejieba其实都是包装了cppjieba（其实还有jiebaR那些也是都用的cppjieba包装起来，性能高）所以性能高。 - 吴延毅  
多模匹配的那个问题，感觉还是之前百度的dictmatch比较适合工程应用，就是建树慢点，查询非常快 - 廖强  
其实不只是从字典下手，还可以从文本入手优化，减少查找字典的次数,理论上定点在一个字，要一直往后分别匹配，直到碰到符号，结尾或达到词的长度范围为止,可以通过词性判断或者反查的方式，跳过一些无意义的词 - tiyee  
词性不靠谱 - 廖强  
我思考用hash也行，而且还简单~ 大概是建一个hash map的词表（偷懒就用memcached），然后我们词语逐个扫描，然后假设我们的词语最长7个字，最短2个字，每次顺序扫描丢给hash map去比对看看是否命中，一次扫描。- Black  
不能解决的问题：多模，还有词距等等,优点是算法简单,至于hash map里是把词语转成一个数字或者是别的降低存储数据量，问题也不大 - Black  
多模，词距 没研究过。。。 我觉得 @吴延毅 写了结巴分词，应该有点心得吧？ - 陈亦  
比如对于一个句子是ABCDE，多模个人觉得最粗暴的办法是遍历这个句子，然后每次都是从trie的root阶段往下找，这样就是对于ABCDE来说，就是从trie的root节点出发找5次。对于词库的词较短（一般2,3个字）的时候，其实这个方法我觉得就够了。但是对于词长较长的时候，ACTrie可以改善性能，其实actrie的原理就是能回溯。但是是在长词的时候，因为在分词中，短词较多。而且本身actrie回溯也引入计算量了。优化效果不明显。长词中我没有试过，不太清楚。 - 吴延毅  
还有一种  
比如“我是中国人，我爱我的祖国”  
那么会匹配  
我，  
我是，  
我是中，  
我是中国  
我是中国人。  
.....  
实际上可以根据某些算法，减少查询次数 - tiyee  
不仅是这样吧？比如类似于正则表达式的搜索：  
从第一个字符开始，依次吃进去一个：  
我  
我是  
我是中  
我是中国  
我是中国人  
...  
然后吐掉第一个字符：  
是  
是中  
是中国  
是中国人 - 陈亦  
比如，基于主语，谓语等，可以消除一些可能性 - tiyee  
@吴延毅 可以讲讲维特比算法啊 - 陈亦  
这个讲起来比较多啊，之前有写过对相关的博文 http://yanyiwu.com/work/2014/04/07/hmm-segment-xiangjie.html （我看访问量还行，也有不少人加我微信反馈说写的比较清楚。）有兴趣的可以参考一下。 - 吴延毅

#### 分享链接

1. 【AC自动机算法详解】  
http://m.blog.csdn.net/article/details?id=6525287&isappinstalled=1

2. 【多模匹配算法与dictmatch实现】  
http://m.blog.csdn.net/article/details?id=9707967&isappinstalled=1

3. 【Trie树优化算法：Double Array Trie 双数组Trie】  
http://m.blog.csdn.net/article/details?id=42526461&isappinstalled=1

4. 【PHP程序员应该知道的15个库】  
http://mp.weixin.qq.com/s?__biz=MzA3NTUzNjk1OA==&mid=402896976&idx=1&sn=cd13c09fe6113a8810f2202588ca781a&scene=1&srcid=030947w9DzAaolyzHg7G6tAf#rd

5. 【七牛首席架构师李道兵:高可用可伸缩架构经验谈】  
http://mp.weixin.qq.com/s?__biz=MzAwNjQwNzU2NQ==&mid=402393179&idx=1&sn=0d030dad340763c71547784769f95913&scene=1&srcid=0309FeQHQgsjMWAgh2zilMGg#rd

6. 【我就是认真，为了一个net.ipv4.tcp_tw_recycle参数】  
http://mp.weixin.qq.com/s?__biz=MzA3MzYwNjQ3NA==&mid=403232978&idx=1&sn=4ed396ac1999add1c866419bd62b0e75&scene=1&srcid=0309A4KyLYRv4T5dBLIUFCGL#rd

7. 【深入浅出CoreOS】  
http://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&mid=401672813&idx=1&sn=125f0aa8e97d1d56520c35a6616b01fd&scene=1&srcid=02190N4FtfnBcEjcZpqiVVeo#rd

8. 【有章可循❳使用HTTP/2提升性能的7个建议】  
http://mp.weixin.qq.com/s?__biz=MzA3NjYxOTA0MQ==&mid=404997797&idx=1&sn=78c8508506eb5c8fc86cb68f72c1f7dc&scene=1&srcid=0309xqLL3fYo1A9x8VYEwtuq#rd
